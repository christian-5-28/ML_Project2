1)
# Embedding
max_features = 400000
max_seq_length = int(sum(numWords)/len(numWords)) + 5
embedding_size = 50

# Convolution
kernel_size = 5
filters = 64
pool_size = 4

# LSTM
lstm_output_size = 64

# Training
batch_size = 50
epochs = 2

model.add(Embedding(max_features, embedding_size, weights=[wordVectors], input_length=max_seq_length))

Dropout before Conv layer and LSTM layer

prevision: 83.3%

score on kaggle: 83.14%

########################################################################################################################

2) TODO

# Embedding
max_features = 400000
max_seq_length = int(sum(numWords)/len(numWords)) + 5
embedding_size = 128

# Convolution
kernel_size = 5
filters = 64
pool_size = 4

# LSTM
lstm_output_size = 64

# Training
batch_size = 50
epochs = 2

TODO
model.add(Embedding(max_features, embedding_size, input_length=max_seq_length))  # w\o wordVectors

Dropout before Conv layer

prevision:

score on kaggle:

########################################################################################################################